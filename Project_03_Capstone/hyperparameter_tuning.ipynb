{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using Hyperdrive\n",
    "\n",
    "This script executes up to a hundred different regression model an hyperparameter combinations to find the best one to predict the effective mortgage a US resident has to pay for a house, an apartment or a trailer. The data is based upon the Microsoft Professional for Data Science Capstone project and was used in a global contest to achieve the highest r2_score where of a score of 0.72 was required to pass the exam.\n",
    "\n",
    "The model training intelligence is stored in the file **CustomModelTraining.py** which can also be executed locally. This script stores the model training script and all it's dependencies in a single folder and then uses Azure HyperDrive to intelligently iterate through a set of hyperparameter combinations on multiple machines in parallel. Each model's outcome, so it's model and it's metrics, are then stored in an archive in the Experiment.\n",
    "\n",
    "This script then chooses the best performing model and uploads it in the Azure Workspace's model zoo so it can be used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing hyper-drive training run for US Mortgage Rate Spread dataset...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import azureml\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import BayesianParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform, choice\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "# add common directory as module search path\n",
    "common_path = os.getcwd()+\"/../common\"\n",
    "if not common_path in sys.path:\n",
    "    sys.path.append(common_path)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ml_principal_authenticate import AzureMLAuthenticator\n",
    "from notebook_check import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset:\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external.\n",
    "\n",
    "Note that the dataset itself is provisioned as AzureML dataset and will directly be fetched from the training script so the code below is only required to fulfull this TODO and to verify the endpoint at the end of this notebook. The data is provisioned in ProvisionDataSets.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None\n",
    "used_dataset = \"EngineeredMortgageSpread\"\n",
    "if used_data_set in ws.datasets.keys(): \n",
    "    print(\"Dataset found, downloading it...\")\n",
    "    dataset = ws.datasets[used_data_set]\n",
    "df = dataset.to_pandas_dataframe()\n",
    "visualize_nb_data(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log into Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connecting to AzureML Workspace...\")\n",
    "service_authenticator = AzureMLAuthenticator(config_path=os.path.normpath(f\"{os.getcwd()}/../Config\"))\n",
    "\n",
    "ws = service_authenticator.get_workspace(\"aml_research\")\n",
    "if ws is not None:\n",
    "    print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "else:\n",
    "    print(\"Workspace not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define script components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dependencies = [\"../common/ml_principal_authenticate.py\", \"../common/notebook_check.py\", \n",
    "                       \"../common/seaborn_vis.py\", \"../Config/ml_principal.json\"]\n",
    "base_directory = os.getcwd()\n",
    "script_file = \"CustomModelTraining.py\"\n",
    "script_path = \"training_script\"\n",
    "local_test_dir = f\"{os.getcwd()}/local_training_script\"\n",
    "local_script_dir = f\"{os.getcwd()}/{script_path}\"\n",
    "print(f\"Training scripts will be stored in {local_script_dir}\")\n",
    "print(f\"Local test run script will be stored in {local_test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wind up compute cluster for hyper drive training execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlcompute_cluster_name = \"tmplphdcluster\"\n",
    "\n",
    "print(f\"Setting up compute cluster... {amlcompute_cluster_name}\")\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    compute_target.update(min_nodes=5, max_nodes=5, idle_seconds_before_scaledown=600)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',# for GPU, use \"STANDARD_NC6\"\n",
    "                                                           vm_priority = 'lowpriority',\n",
    "                                                           min_nodes=5,\n",
    "                                                           max_nodes=5)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count = 5, timeout_in_minutes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble training scripts and test script locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_script_files_in_directory(target_dir):\n",
    "    \"\"\"\n",
    "    Collects all files required for the remote training script execution in the local directory defined\n",
    "    \n",
    "    :param target_dir: The directory in which the script files shall be collected\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shutil.rmtree(target_dir)\n",
    "    except:\n",
    "        pass\n",
    "    os.mkdir(target_dir)\n",
    "    print(f\"Storing training script {script_file} in {target_dir}...\")\n",
    "    shutil.copy(script_file, f\"{target_dir}/{script_file}\")\n",
    "    for dependency in script_dependencies:\n",
    "        print(f\"Storing dependency {dependency}...\")\n",
    "        shutil.copy(f\"{base_directory}/{dependency}\", f\"{target_dir}/{os.path.basename(dependency)}\")\n",
    "    # create place holder for training configuration, is used to tell the script it is packaged\n",
    "    with open(f\"{target_dir}/hd_training_run_config.json\", \"w\") as training_run_file:\n",
    "        pass\n",
    "    print(\"Done\")    \n",
    "    \n",
    "provide_script_files_in_directory(local_test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test script locally before executing it in parallel on HyperDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "os.chdir(local_test_dir)\n",
    "current_python_environment = sys.executable\n",
    "p = Popen([current_python_environment, script_file, \"--models==linear, mlpregressor\", \"--complexity=0.3\"], stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "output, err = p.communicate(b\"input data that is passed to subprocess' stdin\")\n",
    "rc = p.returncode\n",
    "os.chdir(base_directory)\n",
    "print(output.decode(\"utf-8\") )\n",
    "if rc!=0:\n",
    "    print(\"An error occured:\")\n",
    "    print(err.decode(\"utf-8\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare scripts for containerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provide_script_files_in_directory(local_script_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperdrive Configuration\n",
    "\n",
    "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings.\n",
    "\n",
    "I am a using a Bayesian sampling optimization to make the best use of the time, assumming it will quickly detect the strength of the boosting ensembling with quite high complexity grades. Our training script supports overall 3 methods as of now - a simple polynomial base linear regression (linear), a neural network (mlpregressor) and a gradient boosting using a set of estimators. Both the neural network and the gradient boosting are quite strong methods for such complicated problems as our dataset including a lot of binary/categorical factors.\n",
    "\n",
    "In addition I am iterating through several different, reasonable learning rates, **lrf**. As the learning rates for the neural network and the boosting algorithms vary strongly I am providing them as factor to a \"reasonable\" base value defined in the script itself. The effective values chosen will be stored in the output pickle files.\n",
    "\n",
    "Also I am iterating through different **complexity** grades. In case of the neural network they define the width of the hidden neuron layers, in case of the boosting variant they define the depth and count of estimators.\n",
    "\n",
    "And last but not least - though this value only affects the neural network - I am trying two different **iterations** counts.\n",
    "\n",
    "After several tries a max_total_runs value of **50** turned out to be a good compromise, the best result is usually achieved after about 30 runs. Our target metric is - as in the AutoML variant - the r2_score is this was originally also the goal for of the contest this dataset has been used to and a strong indicator for a dataset such as this with a quite huge variance within the label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameter sampler, usnig Baysesian sampling to quickly choose the most promising combinations\n",
    "ps = BayesianParameterSampling( {\n",
    "        \"--model\": choice('linear', 'mlpregressor', 'gradientboosting'),\n",
    "        \"--lrf\": choice(1.0, 0.1, 0.25, 0.5, 2.0),\n",
    "        \"--iterations\": choice(100, 200),\n",
    "        \"--complexity\": choice(1.0, 0.25, 0.5, 2.0)\n",
    "    })\n",
    "\n",
    "# Create a SKLearn estimator for use with train.py\n",
    "est = SKLearn(source_directory=script_path, entry_script=script_file, compute_target=compute_target)\n",
    "# Create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.\n",
    "hyperdrive_config = HyperDriveConfig(estimator=est, hyperparameter_sampling=ps,\n",
    "                            policy=None, primary_metric_name=\"r2_score\",\n",
    "                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                            max_total_runs=50,\n",
    "                            max_concurrent_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup experiment and submit run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'AzureMLCapstoneExperiment_HyperDrive'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "if check_isnotebook():\n",
    "    display(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_run = experiment.submit(hyperdrive_config)\n",
    "if check_isnotebook():\n",
    "    RunDetails(hd_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for completition and archive the best performing model in our model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "best_run = hd_run.get_best_run_by_primary_metric()\n",
    "best_run.register_model('mortgage_prediction_model', f\"outputs/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "TODO: In the cell below, use the RunDetails widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_isnotebook():\n",
    "    from azureml.widgets import RunDetails\n",
    "    RunDetails(best_run).show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning up compute...\")\n",
    "compute_target.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Deployment\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:light,ipynb",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
